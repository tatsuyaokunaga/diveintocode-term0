{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析についてまとめてください。（自分が理解できていることを採点者に伝えてください。）\n",
    "\n",
    "<b>●主成分分析とは</b>  \n",
    "「主成分分析」とは、統計学上のデータ解析手法の一つです。量的な説明変数（因果関係における原因、関数における入力）を、より少ない指標や合成変数（複数の変数が合体したもの）に要約する手法です。この要約を「次元の縮約」と呼ばれることもあります。\n",
    "\n",
    "![次元圧縮](https://cdn-ak.f.st-hatena.com/images/fotolife/u/ut25252/20171019/20171019212457.png)\n",
    "\n",
    "具体的に二次元データを一次元データに圧縮する場合、図のようにデータを二次元平面にプロットしたときに何らかの直線に向かって全データを射影するという方法を考えて見ます。射影した結果、直線の垂直方向の情報が完全に失われ、平行方向の情報のみの一次元データが残ります。このときの分散の値が大きいほど各データの点一つ一つの違いをより多く情報として保っていることになり、最も元データの特徴を保存している良い方向になります。逆にバラつき（分散）が少ない方向というのは、各データが共通して持っている自明な情報なので削除しても問題はないと言えます。\n",
    "まとめると、主成分分析では、データの次元圧縮（縮約）を行う際に、圧縮後のデータの分散が大きくなるような射影をすることで特徴量を自動的に抽出することができるのです。\n",
    "\n",
    "主成分を見つけるためには、分散が最大になるような軸を探します。\n",
    "数学的には以下の流れで行います。\n",
    "1.ある方向に射影した時のデータの分散を計算する。\n",
    "２.分散が最小になるような方向を見つける。\n",
    "結論的には１を行うと共分散行列が出現し、２を行うとその共分散行列に対する固有方程式が得られます。\n",
    "実際にデータを主成分分析する場合は、データから共分散行列を生成し、固有ベクトルを計算、全データを射影するという作業を行います。その結果、固有値は分散の値を表していることが導かれ、固有値の大きい方を第１主成分、固有ベクトルを第１主成分軸とし、以下順番に第２、第３・・・というふうに設定していきます。\n",
    "\n",
    "データの要約という観点からは主成分軸の「寄与率」についての理解が必要になってきます。\n",
    "寄与率とは、下の図のように「この主成分軸一つで、データの何割を説明できているか」と表したものです。  主成分軸と寄与率の関係は、軸の真ん中にデータを射影した場合、データのばらつきがもう一方の軸の持つ寄与率の分減少します。\n",
    "![図3](https://logics-of-blue.com/wp-content/uploads/2017/07/pca-4-intro.jpg)  \n",
    "データが２次元であれば、２つの主成分軸を使えばデータの100%を説明することができます。しかし、データが３次元以上であれば、２つの主成分軸のみだと全てを説明することは不可能です。データのばらつきの特徴をどのくらい説明できているのかを見るときに寄与率は重要です。\n",
    "理論上、主成分の数は変数（データ項目）の数だけ定義できます。何番目までの主成分を採用するかは「寄与率」を基準に判断します。\n",
    "\n",
    "また主成分得点とは、主成分軸を基に、データを回転させた時の座標に相当する値です。第1主成分軸をPC1、第2主成分軸をPC2とすると、下の図のようになります。\n",
    "![図4](https://logics-of-blue.com/wp-content/uploads/2017/07/pca-7-intro.jpg)\n",
    "\n",
    "<b>●主成分分析を用いるケース</b>  \n",
    "主成分分析はマーケティングや研究開発など、さまざまな分野で使われています。\n",
    "例えば以下のような活用法です。\n",
    "-アンケート調査の結果分析で活用\n",
    "-メディア企業や商品評価で活用\n",
    "-研究開発で活用\n",
    "-画像処理で活用\n",
    "\n",
    "<b>●主成分分析で何が嬉しいか</b>  \n",
    "ビッグデータは多変量、多次元であるためそのままでは理解しにくいですが、主成分分析を行うことによって、データの持つ情報をできる限り損なわず、かつデータ全体の雰囲気を可視化し、誰もが理解しやすい形にすることが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析について素人にも分かるように簡潔に説明してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "肥満度を測るためのBMI (Body Mass Index) という指標を考えます。BMIの元のデータは(身長,体重)という2成分を持つデータであったのに対し、BMIはただ1成分の数値となっています。これが次元の圧縮であり、情報を削ぎ落としたにも関わらず、肥満度という特徴を表すのに十分な情報を持っています。このように、データに適切な処理を行えば、情報量の削減と特徴の抽出を同時に行う事ができます。しかし、実際のビックデータでは、BMIの場合のように特徴量があらかじめわかっている場合は少ないです。特徴量とは、データにどのような特徴があるかを数値化したものです。そこで、与えられたデータの傾向から自動的に特徴量を抽出し、その特徴を良く表す低次元データへと次元圧縮を行うのが「主成分分析」です。これは機械学習においては、特に自動的に特徴量を見出すという点において、「教師なし学習」と分類されます。\n",
    "\n",
    "例えば、下の散布図で表現できる、あるデータが与えられたとします。主成分分析とは、そこにデータのばらつきが最大になるような軸を引くことになります。\n",
    "![図１](https://logics-of-blue.com/wp-content/uploads/2017/07/pca-1-intro.jpg)\n",
    "主成分分析を通して引かれた黒くて太い線を主成分軸と呼びます。データがよくばらついている、つまり\n",
    "分散が大きいほど\n",
    "この線はデータのバラつき、すなわち分散が最大になるように引かれています。2本目の線は２番目に幅が広くなるように引かれています。１番目に広いものから順に、第1主成分軸、第2主成分軸・・・と呼ばれています。\n",
    "![図２](https://logics-of-blue.com/wp-content/uploads/2017/07/pca-2-intro.jpg)\n",
    "\n",
    "主成分分析ができると何が嬉しいかというと、データの要約ができ、それによってデータの特徴を判断しやすくなるという利点があります。そのため、データのカテゴリ分けなどにも応用が可能です。\n",
    "例えばユーザーの年齢や収入、購買頻度などなど様々な要素がデータとして蓄えられていて、ユーザーのカテゴライズをしたいと思った時を考えてみます。\n",
    "主成分分析ですと、様々な要素をひとまとめにできるので、例えば「収入が多く購買意欲も高いユーザー」など複数の要素を組み合わせて一つのカテゴリとして扱うことができるようになります。\n",
    "主成分分析を使ったカテゴリ分けは、視覚的にもインパクトのあるものになります。\n",
    "以上のように、データの特徴を見てみたい、と思った際に、主成分分析は有力なツールとなります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析について数式を用いて説明してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二つの変量x、yを持つサンプルデータ（点）が多数あるとする。例えば、x軸が国語の点数、y軸が数学の点数で、点は一人の生徒を表す場合等である。\n",
    "一人一人の違いを見るとき、x軸の国語の側から見るのと（国語の点数を比較する）、y軸の数学の側から見る事が出来る。\n",
    "しかし二つの方向から眺めて一眼で違いを判断することは難しい。またサンプルデータの次元が増えればなおさらのことである。もし生徒の散らばり方の情報を最も保持した新しい一つ次元を減らした軸から見れば、一人一人の違い（各点の離れ方）は一目瞭然である。\n",
    "主成分分析は、この様な線を引く。それは座標軸の回転と考える事も出来る。  \n",
    "このように主成分分析とは、新たな座標軸における各点の座標が最もばらけるように（違いが分かるように）、低次元の座標軸を引くことである。そのような座標軸を引くことを計算で求める。\n",
    "ここで新しい座標軸の単位ベクトルを  $(a,b)=(cosθ,sinθ)$ とする。\n",
    "まず、元の座標でデータの中心化を行い分散を求める。中心化とは、其々の軸の平均値に原点を移動する。分散とは、各データの値$(x_{1},x_{2},・・・ )$から全データの平均値$\\overline{x}$ を引いた値（偏）差の2乗の総和をデータの個数で割ったもの。従って、中心化させる事で各データの値がそのまま偏差となって計算が簡単になる。偏差の2乗を取るのは、原点を挟んでプラスとマイナスがあるので、相殺されるのを防ぐためである。\n",
    "そこで、ある点$k$(中心化後の座標を$x_{k}、y_{k}$とする）と新しい座標軸の単位ベクトルと内積を$D_{k}$とすると、$D_{k}＝ax_{k}＋by_{k} (a=cosθ、b=sinθ)$となる。その2乗は以下のようになる。\n",
    "$$\n",
    "(D_{k})^2＝(ax_{k}＋by_{k})^2 = a^2x_{k}^2+b^2y_{k}^2+2abx_{k}y_{k}\n",
    "$$\n",
    "分散（var）を求めるには、全てのデータの２乗の和を求めてデータの個数$（n）$で割ればいい。以下のように式が展開されるが、$a^2,b^2$は$(a,b)=(cosθ,sinθ)$ で定数であるため括り出せる。\n",
    "\n",
    "$$\n",
    "var=\\frac{1}{n}\\sum_{k=1}^{n}D_{k}^2 =\\frac{1}{n}\\sum_{k=1}^{n}(ax_{k}+by_{k})^2=\\frac{1}{n}\\sum_{k=1}^{n}(a^2x_{k}^2+b^2y_{k}+2abx_{k}y_{k})\n",
    "\\\\=a^2\\frac{1}{n}\\sum_{k=1}^{n}x_{k}^2+b^2\\frac{1}{n}\\sum_{k=1}^{n}y_{k}^2+2ab\\frac{1}{n}\\sum_{k=1}^{n}x_{k}y_{k} ・・・(A)\n",
    "$$\n",
    "(A)式のa、b以外のところはそれぞれ、回転前の座標軸のx座標の分散（中心化後なので平均0の分散）、y座標の分散、x座標y座標の共分散となっている。これを以下のように定める。\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{k=1}^{n}x_{k}^2 => S_{x} : x座標の分散 \\\\\n",
    "\\frac{1}{n}\\sum_{k=1}^{n}y_{k}^2=> S_{y} : y座標の分散  \\\\\n",
    "\\frac{1}{n}\\sum_{k=1}^{n}x_{k}y_{k} => S_{xy} : x,y座標共分散\\\\\n",
    "$$\n",
    "また、$a＝cosθ、b＝sinθ$から、$a^2＋b^2＝1$の制約もある。\n",
    "この制約の中で、分散varの最大値を求めるために、ラグランジュの未定係数法を用いる。\n",
    "この方法によれば、以下のように関数を作り、Gの最大値を与える$a、b、λ$を求めれば、Fの最大値を与える$a、b$も求まることが分かっている。\n",
    "\n",
    "$$\n",
    "F(a,b)=S_{x}a^2+S_{y}b^2+S_{xy}2ab\\\\\n",
    "C(a,b)=a^2+b^2-1=0\\\\\n",
    "G(a,b,λ) = F(a,b)-λC(a,b)\\\\\n",
    "$$\n",
    "これを解くには、$G$を$a、b、λ$で其々偏微分して、＝0と置いた連立方程式を作る。\n",
    "\n",
    "$$\n",
    "G(a,b,λ) = F(a,b)-\\lambda C(a,b)=S_{x}a^2+S_{y}b^2+S_{xy}2ab-λ(a^2+b^2-1)\\\\\n",
    "\\frac{∂G}{∂a}=2S_{x}a+2S_{xy}b-2λa=0\\\\\n",
    "\\frac{∂G}{∂b}=2S_{y}b+2S_{xy}a-2λb=0\\\\\n",
    "\\frac{∂G}{∂λ}=-a^2-b^2+1=0\n",
    "$$\n",
    "\n",
    "上記の偏微分した式をまとめると\n",
    "$$\n",
    "S_{x}a+S_{xy}b=λa　・・・(1)  \\\\ \n",
    "S_{y}b+S_{xy}a=λb ・・・(2) \\\\\n",
    "a^2+b^2=1\\\\\n",
    "$$\n",
    "$$\n",
    "{\\begin{pmatrix}\n",
    "S_{x} &S_{xy} \\\\\n",
    "S_{xy} & S_{y} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{pmatrix}=\n",
    "}\n",
    "λ\n",
    "\\begin{pmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "この式は共分散行列\n",
    "\\begin{pmatrix}\n",
    "S_{x} &S_{xy} \\\\\n",
    "S_{xy} & S_{y} \n",
    "\\end{pmatrix}\n",
    "の固有方程式になっている。\n",
    "以上から、λは共分散行列の固有値、(a、b)はその固有ベクトルになっているのでそれを求めればよい。\n",
    "固有ベクトルは通常、a、bの比しか求められないが、ここでは$a^2＋b^2＝1$の制約があるので、各固有値に対する$a、b$は一意に決まる。\n",
    "    \n",
    "分散varを最大化するa、b、λはラグランジュの未定係数法により、以下の(1)(2)のように求められた。(1)×a＋(2)×bと置くと、これも＝0となる。\n",
    "これを整理して、$a^2＋b^2＝1$の条件を使うと\n",
    "$$\n",
    "S_{x}a+S_{xy}b-λa=0　・・・(1)\\\\\n",
    "S_{y}b+S_{xy}a-λb=0　 ・・・(2)\\\\\n",
    "$$\n",
    "\n",
    "$(1) \\times a+(2) \\times b$は\n",
    "$$\n",
    "S_{x}a^2+S_{y}b^2+2S_{xy}ab-λ(a^2+b^2)=0\\\\\n",
    "\\Leftrightarrow \n",
    "S_{x}a^2+S_{y}b^2+2S_{xy}ab=λ\\\\\n",
    "$$\n",
    "となり、左辺は最大化を目指した主成分得点の分散var（A）に他ならず、varの値は固有値λそのものであることを示してる。このように主成分分析は固有値問題に帰結することがわかる。\n",
    "各固有値の大きさが、その軸の主成分得点の分散の大きさを表す。それが大きいほど、全体のデータの特徴を、一方向からよく眺められることになり主成分分析の目的に合致する。大きい固有値の固有ベクトルを第１主成分、二番めの大きさの固有値の固有ベクトルを第２主成分と呼ぶ。\n",
    "対称行列の固有ベクトルは互いに直交する。共分散行列は対称行列なので、その固有ベクトルの第１主成分と第２主成分は直交する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析をPythonで実装してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# 主成分分析をするライブラリ\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_scratch(data):\n",
    "    '''\n",
    "    自作の主成分分析を行う関数\n",
    "    n次元から2次元のデータに圧縮する。\n",
    "    引数：元データ　（n次元）行（データ数）列の行列\n",
    "    返り値：２次元に圧縮されたデータの２行（データ数）列の行列\n",
    "    '''\n",
    "    # データの偏差を求める\n",
    "    data_deviation = np.array([row - np.mean(row) for row in data.transpose()]).transpose()\n",
    "    \n",
    "    # 分散共分散行列を求める\n",
    "    cov_array = np.cov(data_deviation.T)\n",
    "\n",
    "    '''\n",
    "    cov関数を使用しないなら下記のようになる\n",
    "        X_bar = np.array([row - np.mean(row)\\\n",
    "                                                   for row in X.transpose()]).transpose()\n",
    "        cov_array = np.dot(X_bar.T, X_bar) / (X.shape[0] - 1)\n",
    "    '''\n",
    "    \n",
    "    print(\"スクラッチ分散共分散行列\")\n",
    "    print(cov_array)\n",
    "    # 上の分散共分散行列を用いて固有値、固有ベクトルを求める\n",
    "    lam,eigen_vecter = np.linalg.eig(cov_array)\n",
    "    print(\"スクラッチ\")\n",
    "    print(lam)\n",
    "    print('スクラッチ固有ベクトル')\n",
    "    print(eigen_vecter)\n",
    "    \n",
    "    # np.linalg.eig関数では固有値順にソートされていないため\n",
    "    #　固有ベクトルを固有値の大きい順に並べ換える\n",
    "    lam_index = [n for n in range(len(lam))]\n",
    "    for i in range(len(lam)):\n",
    "        for j in range(i + 1,len(lam)):\n",
    "            if lam[i] < lam[j]:\n",
    "                lam[i],lam[j] = lam[j],lam[i]\n",
    "                lam_index[i],lam_index[j] = lam_index[j],lam_index[i]\n",
    "                \n",
    "    print('スクラッチ第一主成分の寄与率')\n",
    "    print(lam[0] / sum(lam))\n",
    "    \n",
    "    # 各データの第一主成分の値を計算\n",
    "    first_axes = np.dot(eigen_vecter[:, lam_index[0]].T,data.T)\n",
    "    # 各データの第二主成分の値を計算\n",
    "    second_axes = np.dot(eigen_vecter[:, lam_index[1]].T,data.T)\n",
    "    \n",
    "    return np.array([first_axes, second_axes])\n",
    "\n",
    "# PCAを使用したお手本コード\n",
    "#[pythonのコードのスター総数、Javaのコードのスターの総数、年収]\n",
    "X = np.array([[70,30,700],[32,60,480],[32,20,300],[20,120,600],\n",
    "                          [40,120,630],[40,30,520],[300,1100,1200],[2000,400,1500],[40,180,800]])\n",
    "# n_componentsで削減先の次元を２次元に指定\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X) # 主成分分析を実行\n",
    "\n",
    "\n",
    "#データの確認\n",
    "print('ライブラリの固有値')\n",
    "print(pca.explained_variance_)\n",
    "print('ライブラリ固有ベクトル')\n",
    "print(pca.components_)\n",
    "print('ライブラリ分散共分散行列')\n",
    "print(pca.get_covariance())\n",
    "print('ライブラリ累積寄与率')\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "print('###########')\n",
    "\n",
    "# 次元削減をXに適用する。\n",
    "pca_point = pca.transform(X)\n",
    "    \n",
    "\n",
    "# スクラッチ関数で圧縮したデータも用意する\n",
    "pca_point2 = pca_scratch(X)\n",
    "\n",
    "#スクラッチ関数で作った圧縮データは青でライブラリ関数で作った圧縮データは赤でプロットして結果を確認する\n",
    "plt.scatter(*pca_point.T, color = 'red')\n",
    "plt.scatter(pca_point2[0],pca_point2[1], color = 'blue')\n",
    "#グラフを表示する\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## その他、今回の授業で学んだことを記述してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ナイーズベイスを初めて知りました。迷惑メールの分離にも応用されているそうです。もっと詳しい説明を聞きたかったのですが、時間切れで残念でした。\n",
    "ベイズ理論が応用された手法に興味を持ちました。主成分分析は理解が授業時間内では十分できていませんでしたが、課題をしていく中で腹に落ちてきた感じです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
