{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 課題　ボストン住宅価格　線形回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形回帰とは何か"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以下の観点を全て含めて記述しましょう。\n",
    "- 線形回帰とは何か。  \n",
    "- 具体的に言うと？  \n",
    "- 分類とは何か違うのか。 　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え：  \n",
    "コンピュータに与える扱う対象の特徴を現した量のことを「特徴量」といいます。そして、そのデータの属性、カテゴリを表す識別子のことを「ラベル」と言います。\n",
    "コンピュータに特徴量（データ）を与える際に、その特徴量が示すカテゴリや数値も一緒に与えて、データが属するカテゴリーや数値を予測する機械学習を教師あり学習といいます。教師あり学習は大きく「回帰」と「分類」に分けられ、前者の「回帰」の方法の一つに「線形回帰」があります。\n",
    "　　まず回帰とはYが連続値のときに、データY=f(X)というモデルをあてはめることをいいます。この変数間に一方のXが他方のYに左右ないし決定する影響があるとき、Xと説明変数（独立変数）、Yを目的変数（従属変数）といいます。\n",
    "そのうち直線で回帰することを「線形回帰」、直線以外の滑らかな線で回帰することを「非線形回帰」といいます。Xが１次元なら単回帰、Xが２次元以上ならば重回帰といいます。\n",
    "具体的には\n",
    "\n",
    "一方、Yが連続値ではなく、離散の場合を「分類」といいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要なライブラリをimportする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston= load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得データをDataFrameにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 説明変数を’LSTAT’のみにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#X=X['LSTAT'].values\n",
    "X=X['LSTAT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 単回帰と重回帰についての違いを記述せ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単回帰では、１つの結果変数に対して影響を与える説明変数はが１つであるという、ある種の前提を採用しているのに対し、\n",
    "これに対して、重回帰では、１つの結果変数に影響を与える説明変数は複数存在するであろうという前提を採用している点が、最も大きく違うところです。\n",
    "また、重回帰では複数の説明変数を前提とすることから、どの説明変数が最も大きな影響を結果変数に与えているかという「説明変数間の影響度の比較」を実施することが可能だという点も大きな特徴です。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.98\n",
       "1    9.14\n",
       "2    4.03\n",
       "3    2.94\n",
       "4    5.33\n",
       "Name: LSTAT, dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  24.0\n",
       "1  21.6\n",
       "2  34.7\n",
       "3  33.4\n",
       "4  36.2"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータに分割する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_1d = LinearRegression()\n",
    "lin_1d.fit(X_train[:,None],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 決定係数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一次式における 'LSTAT'の住宅係数は0.57\n"
     ]
    }
   ],
   "source": [
    "score_1d=lin_1d.score(X_train[:,np.newaxis],y_train)\n",
    "print(\"一次式における 'LSTAT'の住宅係数は%.2f\"%(score_1d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 決定係数は何か記述せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決定係数は回帰モデルの予測によって、説明変数が目的変数をどれくらい説明できているか（回帰分析の精度）を表す指標です。\n",
    "または、回帰モデルが学習用に与えられたデータで成績が良いだけでなく、新規の未知のデータに対しても良い成績を出せる（汎化能力）かどうかを測る尺度のことです。決定係数は一般に$R^2$と示され、０から１までの値をとります。１に近いほど回帰式が実際のデータに当てはまっていることを表しており、説明変数が目的変数をよく説明していると言えます。したがって、もっとも説明変数が、目的変数を説明できる場合、決定係数は１になります。\n",
    "\n",
    "決定係数を求めるためには、実際のデータと推定された回帰式から「全変動」「回帰変動」「残渣変動」の３つを求める必要があります。ここでは実際のデータを$(x_i,y_i)$、回帰式から推定されたデータを$(x_i,\\hat{y}_i)$、データ全体から求められる平均値を$(\\bar{x},\\bar{y})$とします。\n",
    "![決定係数](http://bellcurve.jp/statistics/wp-body/wp-content/uploads/2017/01/795316b92fc766b0181f6fef074f03fa-16.png)\n",
    "\n",
    "- 「全変動」：実際のデータとデータ全体の平均値との差を表す。（上の図の緑の部分）\n",
    "- 「回帰変動」：推定された回帰式から得られた予測値とデータ全体の平均値の差を表す（上の図の紫の部分）\n",
    "- 「残差変動」：実際のデータと推定された回帰式から得られた予測値との差を表す。（上の図の赤の部分）\n",
    "これらの変動を二乗和として算出します。\n",
    "①全変動の二乗和   $$ \\sum_{i=1}^{n} (y_i-\\bar{y})^2 $$\n",
    "②回転変動の二乗和　$$\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2 $$\n",
    "③残差変動の二乗和　$$\\sum_{i=1}^{n}(\\hat{y_i}-\\bar{y})^2$$\n",
    "\n",
    "上の図から　　　「全変動」 = 「回帰変動」 + 「残差変動」  がなりたちます。　\n",
    "\n",
    "決定係数は説明変数が目的変数をどれくらい説明しているか、つまり「回帰変動が全変動に対してどれだけ多いか＝残差変動が全変動に対してどれだけ少ないか」を表すものです。したがって決定係数は、次に示すように回帰変動を全変動で割ることになります。\n",
    "\n",
    "$$\n",
    "R^2=\\frac{\\sum_{i=1}^{n}(\\hat{y_i}-\\bar{y})^2}{\\sum_{i=1}^{n} (y_i-\\bar{y})^2}\n",
    "=1-\\frac{\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i-\\bar{y})^2}\n",
    "$$\n",
    "\n",
    "\n",
    ">https://bellcurve.jp/statistics/course/9706.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 決定係数をいかなる場合も信じても良いか記述せよ(決定係数が高ければ、汎用性があるモデルと言えるか)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必ずしもそうとは言えません。\n",
    "大変大きな母集団（日本全国民など）から適当に取り出された一部の標本集団を用いて回帰分析を行い、決定係数を出したとします。そこで求められた回帰係数はその一部の標本集団に対して最も予測が当たるように求められたものであるため、それよりはるかに大きい母集団に対してこれを適用すると受けて予測の精度が悪くなることが考えられます。先の決定係数は標本集団への予測の当てはまりだけを見ている指標であるため、それよりはるかに大きな母集団に対しこの決定係数を当てはめようとすると、予測の当てはまりの良さを課題に見積もっていることになり不適切になります。\n",
    "こうした問題に対処し、あくまで母集団の方を対象とした予測値の当てはまりを評価できるような決定係数の値に修正したものが「自由度調整済み決定係数」です。値の修正にはサンプル数と説明変数の個数から求められる「自由度」という数字を使っているためこの名前がつけられています。\n",
    "統計的に、一部の適当に選ばれた標本集団の性質よりも母集団の性質の方を知りたい場合が通常であるから、決定係数よりも自由度調整済み決定係数の方が好ましい性質を持っていると言えます。\n",
    "しかし、「自由度調整済み決定係数さえ高ければなんでもいい」という考え方も不適切です。\n",
    "一般的に説明変数を増やすと決定係数も１に近づいていくため、分析の目的と仮説とは一切関係のない説明変数を決定係数が上がるからという理由で加えてしまうと、そのモデルは解釈が難しくなり、。当初の目的を果たせなくなります。\n",
    "また、決定係数はあくまで「予測の当てはまりの良さ」を表す指標であるため、分析の目的が、ある変数の値を予測したいという場合には適切な指標であるが、ある変数の影響の有無が主眼であり予測は重視しない場合には決定係数に注目することはあまり意味がない。その場合は注目する説明変数の回帰直線の傾きや分析の目的と照らし合わせて正しく使うことが求められる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2,3,4次式の回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二次式における'LSTAT'の住宅価格への決定係数は0.52\n",
      "三次式における'LSTAT'の住宅価格への決定係数は0.54\n",
      "四次式における'LSTAT'の住宅価格への決定係数は0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "degree_2 = PolynomialFeatures(degree=2)\n",
    "degree_3 = PolynomialFeatures(degree=3)\n",
    "degree_4 = PolynomialFeatures(degree=4)\n",
    "\n",
    "x_train_2 = degree_2.fit_transform(X_train[:,None])\n",
    "x_train_3 = degree_3.fit_transform(X_train[:,None])\n",
    "x_train_4 = degree_4.fit_transform(X_train[:,None])\n",
    "\n",
    "lin_2d = LinearRegression()\n",
    "lin_3d = LinearRegression()\n",
    "lin_4d = LinearRegression()\n",
    "\n",
    "lin_2d.fit(x_train_2,y_train)\n",
    "lin_3d.fit(x_train_3,y_train)\n",
    "lin_4d.fit(x_train_4,y_train)\n",
    "\n",
    "x_test_2 = degree_2.fit_transform(X_test[:,np.newaxis])\n",
    "x_test_3 = degree_3.fit_transform(X_test[:,np.newaxis])\n",
    "x_test_4 = degree_4.fit_transform(X_test[:,np.newaxis])\n",
    "\n",
    "score_2d = lin_2d.score(x_test_2,y_test)\n",
    "score_3d = lin_3d.score(x_test_3,y_test)\n",
    "score_4d = lin_4d.score(x_test_4,y_test)\n",
    "\n",
    "print(\"二次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_2d))\n",
    "print(\"三次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_3d))\n",
    "print(\"四次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_4d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次数が大きくなるとどうなるか記述せよ\n",
    "\n",
    "以下の観点をすべて含めて記述しましょう。\n",
    "\n",
    "- 説明変数をxとして、次数を増やしていくとどのように数式が変化していくか記述せよ（1次式 ax + b）\n",
    "- 次数を増やすとどのようなメリットが考えられるか\n",
    "- 次数を増やすとどのようなデメリットが考えられるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "説明変数をxとして誤差関数の次数を増やしていくと数式は以下のように変化していきます。  \n",
    "１次式は　$ax+b(a\\neq0)$ 、2次式は $ax^2+bx+c(a\\neq0)$、３次式は　$ax^3+bx^2+cx+d(a\\neq0) $、４次式は$ax^4+bx^3+cx^2+dx+e()a\\neq0$・・・というように変化していきます。\n",
    "また、次数を上げてくほど、表現力が向上し決定係数は１により近づいていくため、回帰分析の精度が向上していくというメリットが考えられます。\n",
    "一方、次数を増やすほどモデルの表現力が高くなっていくためにデータに含まれるノイズについても無理に学習してしまうという過学習が発生するというデメリットが考えられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 重回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# カラムを表示\n",
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二次式における'LSTAT'の住宅価格への決定係数は0.69\n",
      "三次式における'LSTAT'の住宅価格への決定係数は0.72\n",
      "四次式における'LSTAT'の住宅価格への決定係数は0.26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston= load_boston()\n",
    "\n",
    "X=pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target)\n",
    "\n",
    "X=X.loc[:, ['LSTAT','CRIM','RM','RAD']]\n",
    "#X=X['LSTAT']['RAD'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "degree_2 = PolynomialFeatures(degree=2)\n",
    "degree_3 = PolynomialFeatures(degree=3)\n",
    "degree_4 = PolynomialFeatures(degree=4)\n",
    "\n",
    "x_train_2 = degree_2.fit_transform(X_train)\n",
    "x_train_3 = degree_3.fit_transform(X_train)\n",
    "x_train_4 = degree_4.fit_transform(X_train)\n",
    "\n",
    "lin_2d = LinearRegression()\n",
    "lin_3d = LinearRegression()\n",
    "lin_4d = LinearRegression()\n",
    "\n",
    "lin_2d.fit(x_train_2,y_train)\n",
    "lin_3d.fit(x_train_3,y_train)\n",
    "lin_4d.fit(x_train_4,y_train)\n",
    "\n",
    "x_test_2 = degree_2.fit_transform(X_test)\n",
    "x_test_3 = degree_3.fit_transform(X_test)\n",
    "x_test_4 = degree_4.fit_transform(X_test)\n",
    "\n",
    "score_2d = lin_2d.score(x_test_2,y_test)\n",
    "score_3d = lin_3d.score(x_test_3,y_test)\n",
    "score_4d = lin_4d.score(x_test_4,y_test)\n",
    "\n",
    "print(\"二次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_2d))\n",
    "print(\"三次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_3d))\n",
    "print(\"四次式における'LSTAT'の住宅価格への決定係数は%.2f\"%(score_4d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>以上から説明変数を'LSTAT','CRIM','RM','RAD'の四つを組み合せの場合、３次誤差関数の決定係数が0.72になります。</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重回帰について記述せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数の特徴量を使う回帰を重回帰という。\n",
    "特徴量を増やせば、誤差関数の誤差が小さくなり、決定係数の値をより高めることが可能で、予測の精度をより向上させることができるというメリットがある。\n",
    "ただデメリットとしては一つ前の問題で４次式の決定係数が0.26と３次式のそれよりはるかに値が下がっていることから、説明変数を増やし、次数をあげると精度と表現力が向上する一方で過学習がおこってしまうというメデリットがある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
